---
title: "A comparison of methods for predicting clothing classes using the Fashion MNIST dataset in RStudio and Python (Part 1)"
author: "Florianne Verkroost"
date: "23/08/2019"
output: html_document
---
  
In this series of blog posts, I will compare different machine and deep learning methods to predict clothing categories from images using the Fashion MNIST data. In this first blog of the series, we will explore and prepare the data for analysis. I will also show you how to predict the clothing categories of the Fashion MNIST data using my go-to model: an artificial neural network. To show you how to use one of RStudios incredible features to run Python from RStudio, I build my neural network in Python. In the [second blog post](https://github.com/fverkroost/RStudio-Blogs/blob/master/machineLearningFashionMNIST2.Rmd), we will experiment with tree-based methods (single tree, random forests and boosting) and support vector machines to see whether we can beat the neural network in terms of performance. 

```{r setup, message = FALSE, warning = FALSE, results = 'hide', echo = FALSE}
python_path <- "/Library/Frameworks/Python.framework/Versions/3.7/bin/python3"
knitr::opts_chunk$set(echo = TRUE, engine.path = list(python = python_path))
library(reticulate)
use_python(python_path)
```

To start, we first set our seed to make sure the results are reproducible.
```{r, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(1234)
```

## Importing and exploring the data

The `keras` package contains the Fashion MNIST data, so we can easily import the data into RStudio from this package directly after installing it from Github and loading it.

```{r, message = FALSE, warning = FALSE, results = 'hide'}
library(devtools)
devtools::install_github("rstudio/keras")
library(keras)        
install_keras()  
fashion_mnist <- keras::dataset_fashion_mnist()
```

The resulting object named `fashion_mnist` is a nested list, consisting of lists `train` and `test`. Each of these lists in turn consists of arrays `x` and `y`. To look at the dimentions of these elements, we recursively apply the `dim()` function to the `fashion_mnist` list.

```{r}
rapply(fashion_mnist, dim)
```

From the result, we observe that the `x` array in the training data contains `r dim(fashion_mnist$train$x)[3]` matrices each of `r nrow(fashion_mnist$train$x)` rows and `r ncol(fashion_mnist$train$x)` columns, or in other words `r nrow(fashion_mnist$train$x)` images each of `r ncol(fashion_mnist$train$x)` by `r dim(fashion_mnist$train$x)[3]` pixels. The `y` array in the training data contains `r nrow(fashion_mnist$train$y)` labels for each of the images in the `x` array of the training data. The test data has a similar structure but only contains `r nrow(fashion_mnist$test$x)` images rather than `r nrow(fashion_mnist$train$x)`. For simplicity, we rename these lists elements to something more intuitive (where `x` now represents images and `y` represents labels):
  
```{r}
c(train.images, train.labels) %<-% fashion_mnist$train
c(test.images, test.labels) %<-% fashion_mnist$test
```

Every image is captured by a `r ncol(fashion_mnist$train$x)` by `r dim(fashion_mnist$train$x)[3]` matrix, where entry [i, j] represents the opacity of that pixel on an integer scale from `r min(fashion_mnist$train$x)` (white) to `r max(fashion_mnist$train$x)` (black). The labels consist of integers between zero and nine, each representing a unique clothing category. As the category names are not contained in the data itself, we have to store and add them manually. Note that the categories are evenly distributed in the data.

```{r}
cloth_cats = data.frame(category = c('Top', 'Trouser', 'Pullover', 'Dress', 'Coat',  
                                     'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Boot'), 
                        label = seq(0, 9))
```

To get an idea of what the data entail and look like, we plot the first ten images of the test data. To do so, we first need to reshape the data slightly such that it becomes compatible with `ggplot2`. We select the first ten test images, convert them to data frames, rename the columns into digits 1 to `r ncol(fashion_mnist$train$x)`, create a variable named `y` with digits 1 to `r ncol(fashion_mnist$train$x)` and then we melt by variable `y`. We need package `reshape2` to access the `melt()` function. This results in a `r ncol(fashion_mnist$train$x)` times `r ncol(fashion_mnist$train$x)` equals `r ncol(fashion_mnist$train$x)*ncol(fashion_mnist$train$x)` by 3 (y pixels (= y), x pixels (= variable) and the opacity (= value)) data frame. We bind these all together by rows using the `rbind.fill()` function from the `plyr` package and add a variable `Image`, which is a unique string repeated `r ncol(fashion_mnist$train$x)*ncol(fashion_mnist$train$x)` times for each of the nine images containing the image number and corresponding test set label.

```{r, message = FALSE, warning = FALSE, results = 'hide'}
library(reshape2)
library(plyr)
subarray <- apply(test.images[1:10, , ], 1, as.data.frame)
subarray <- lapply(subarray, function(df){
  colnames(df) <- seq_len(ncol(df))
  df['y'] <- seq_len(nrow(df))
  df <- melt(df, id = 'y')
  return(df)
})
plotdf <- rbind.fill(subarray)
first_ten_labels <- cloth_cats$category[match(test.labels[1:10], cloth_cats$label)]
first_ten_categories <- paste0('Image ', 1:10, ': ', first_ten_labels)
plotdf['Image'] <- rep(first_ten_categories, unlist(lapply(subarray, nrow)))
```

We then plot these first ten test images using package `ggplot2`. Note that we reverse the scale of the y-axis because the original dataset contains the images upside-down. We further remove the legend and axis labels and change the tick labels.

```{r, message = FALSE, warning = FALSE, results = 'hide'}
library(ggplot2)
```

```{r}
ggplot() + 
  geom_raster(data = plotdf, aes(x = variable, y = y, fill = value)) + 
  facet_wrap(~ Image, nrow = 2, ncol = 5) + 
  scale_fill_gradient(low = "white", high = "black", na.value = NA) + 
  theme(aspect.ratio = 1, legend.position = "none") + 
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(breaks = seq(0, 28, 7), expand = c(0, 0)) + 
  scale_y_reverse(breaks = seq(0, 28, 7), expand = c(0, 0))
```

## Data Preparation

Next, it's time to start the more technical work of predicting the labels from the image data. First, we need to reshape our data to convert it from a multidimensional array into a two-dimensional matrix. To do so, we vectorize each `r ncol(fashion_mnist$train$x)` by `r ncol(fashion_mnist$train$x)` matrix into a column of length `r ncol(fashion_mnist$train$x)*ncol(fashion_mnist$train$x)`, and then we bind the columns for all images on top of each other, finally taking the transpose of the resulting matrix. This way, we can convert a `r ncol(fashion_mnist$train$x)` by `r ncol(fashion_mnist$train$x)` by `r nrow(fashion_mnist$train$x)` array into a `r nrow(fashion_mnist$train$x)` by `r ncol(fashion_mnist$train$x)*ncol(fashion_mnist$train$x)` matrix. We also normalize the data by dividing between the maximum opacity of `r max(fashion_mnist$train$x)`.

```{r}
train.images <- data.frame(t(apply(train.images, 1, c))) / max(fashion_mnist$train$x)
test.images <- data.frame(t(apply(test.images, 1, c))) / max(fashion_mnist$train$x)
```

We also create two data frames that include all training and test data (images and labels), respectively.

```{r}
pixs <- 1:ncol(fashion_mnist$train$x)^2
names(train.images) <- names(test.images) <- paste0('pixel', pixs)
train.labels <- data.frame(label = factor(train.labels))
test.labels <- data.frame(label = factor(test.labels))
train.data <- cbind(train.labels, train.images)
test.data <- cbind(test.labels, test.images)
```

## Neural Network

Now, let's continue by building a simple neural network model to predict our clothing categories. Neural networks are artificial computing systems that were built with human neural networks in mind. Neural networks contain nodes, which transmit signals amongst one another. Usually the input at each node is a number, which is transformed according to a non-linear function of the input and weights, the latter being the parameters that are tuned while training the model. Sets of neurons are collected in different layers; neural networks are reffered to as 'deep' when they contain at least two hidden layers. 

Although neural networks can easily built in RStudio using Tensorflow and Keras, I really want to show you one of the incredible features of RStudio where you can run Python in RStudio. This can be done in two ways: either we choose "Terminal" on the top of the output console in RStudio and run Python via Terminal, or we use the base `system2()` function to run Python in RStudio. 

For the second option, to use the `system2()` command, it's important to first check what version of Python should be used. You can check which versions of Python are installed on your machine by running `python --version` in Terminal. Note that with RStudio 1.1 (1.1.383 or higher), you can run in Terminal directly from RStudio on the "Terminal" tab. You can also run `python3 --version` to check if you have Python version 3 installed. On my machine, `python --version` and `python3 --version` return Python 2.7.16 and Python 3.7.0, respectively. You can then run `which python` (or `which python3` if you have Python version 3 installed) in Terminal, which will return the path where Python is installed. In my case, these respective commands return `/usr/bin/python` and `/Library/Frameworks/Python.framework/Versions/3.7/bin/python3`. As I will make use of Python version 3, I specify the latter as the path to Python in the `use_python()` function from the `reticulate` package. We can check whether the desired version of Python is used by using the `sys` package from Python. Just make sure to change the path in the code below to what version of Python you desire using and where that version in installed.

```{r, eval = FALSE}
library(reticulate)
use_python(python = '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3')
```

```{r}
sys <- import("sys")
sys$version
```

Now that we've specified the correct version of Python to be used, we can run our Python script from RStudio using the `system2()` function. This function also takes an argument for the version of Python used, which in my case is Python version 3. If you are using an older version of Python, make sure to change `"python3"` in the command below to `"python2"`.

```{r, eval = FALSE}
python_file <- "simple_neural_network_fashion_mnist.py"
system2("python3", args = c(python_file), stdout = NULL, stderr = "")
```

I will now guide you step by step through the script called in the command above. First, we load the required packages in Python and we load and prepare the data.

```{python, engine.path = '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3', message = FALSE, warning = FALSE, results = 'hide'}
# Load required packages
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.applications import VGG19
from keras.applications.vgg19 import preprocess_input
from keras.models import Model
from keras import models
from keras import layers
from keras import optimizers

# Load the train and test images and label from the Fashion MNIST data from keras
fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Normalize the data by dividing by maximum opacity of 255
train_images = train_images / len(train_images)
test_images = test_images / len(test_images)
```

We start by building a simple neural network containing one hidden layer. Note that as here we use the untransformed but normalized data, we need to flatten the `r ncol(fashion_mnist$train$x)` by `r ncol(fashion_mnist$train$x)` pixels input first. After building the neural network, we compile it by specifying a loss function suitable for categorical multi-class responses, an optimizer and our metric of interest. Hereafter, we can train the model onto our training data set.

```{python, message = FALSE, warning = FALSE, results = 'hide'}
# Develop a model with three layers (input layer, hidden layer and output layer)
three_layer_model = Sequential()
three_layer_model.add(Flatten(input_shape = (28, 28)))
three_layer_model.add(Dense(128, activation = 'relu'))
three_layer_model.add(Dense(10, activation = 'softmax'))

# Compile and fit the model onto the training data
three_layer_model.compile(loss = 'sparse_categorical_crossentropy', 
                          optimizer = 'adam', metrics = ['accuracy'])
three_layer_model.fit(train_images, train_labels, epochs = 5, validation_split = 0.3)
```

```{python}
# Compute the accuracy and loss of model performance on the test data
test_loss, test_acc = three_layer_model.evaluate(test_images, test_labels)
print("Model with three layers and five epochs -- Test loss:", test_loss * 100)
print("Model with three layers and five epochs -- Test accuracy:", test_acc * 100)
```

We can see that the neural network with one hidden layer already performs relatively well compared to the tree-based methods we've seen before, with a test set accuracy of `python test_acc` percent. To see whether a deep neural network performs better at predicting clothing categories, we build a neural network with three hidden layers in a similar way as before.

```{python, message = FALSE, warning = FALSE, results = 'hide'}
# Develop a model with five layers (input layer, five hidden layers and output layer)
five_layer_model = Sequential()
five_layer_model.add(Flatten(input_shape = (28, 28)))
five_layer_model.add(Dense(128, activation = 'relu'))
five_layer_model.add(Dense(128, activation = 'relu'))
five_layer_model.add(Dense(128, activation = 'relu'))
five_layer_model.add(Dense(10, activation = 'softmax'))

# Compile and fit the model onto the training data
five_layer_model.compile(loss = 'sparse_categorical_crossentropy', 
                         optimizer = 'adam', metrics = ['accuracy'])
five_layer_model.fit(train_images, train_labels, epochs = 5, validation_split = 0.3)
```

```{python}
# Compute the accuracy and loss of model performance on the test data
test_loss, test_acc = five_layer_model.evaluate(test_images, test_labels)
print("Model with five layers and five epochs -- Test loss:", test_loss * 100)
print("Model with five layers and five epochs -- Test accuracy:", test_acc * 100)
```
The model with two additional hidden layers performs slightly better and has lower loss than the model with one hidden layer. Let's try whether adding another five hidden layers improves model performance even more.

```{python, message = FALSE, warning = FALSE, results = 'hide'}
# Develop a model with ten layers (input layer, eight hidden layers and output layer)
ten_layer_model = Sequential()
ten_layer_model.add(Flatten(input_shape = (28, 28)))
ten_layer_model.add(Dense(128, activation = 'relu'))
ten_layer_model.add(Dense(128, activation = 'relu'))
ten_layer_model.add(Dense(128, activation = 'relu'))
ten_layer_model.add(Dense(128, activation = 'relu'))
ten_layer_model.add(Dense(128, activation = 'relu'))
ten_layer_model.add(Dense(128, activation = 'relu'))
ten_layer_model.add(Dense(128, activation = 'relu'))
ten_layer_model.add(Dense(128, activation = 'relu'))
ten_layer_model.add(Dense(10, activation = 'softmax'))

# Compile and fit the model onto the training data
ten_layer_model.compile(loss = 'sparse_categorical_crossentropy', 
                        optimizer = 'adam', metrics = ['accuracy'])
ten_layer_model.fit(train_images, train_labels, epochs = 5, validation_split = 0.3)
```

```{python}
# Compute the accuracy and loss of model performance on the test data
test_loss, test_acc = ten_layer_model.evaluate(test_images, test_labels)
print("Model with ten layers and five epochs -- Test loss:", test_loss * 100)
print("Model with ten layers and five epochs -- Test accuracy:", test_acc * 100)
```

Clearly, of the neural networks developed here, the one with three hidden layers works best. The deeper model with eight hidden layers seems to hardly improve accuracy while increasing loss. Let's try and see whether increasing the number of epochs (i.e. the number of times the model iterates through the training data) from five to fifty.

```{python, message = FALSE, warning = FALSE, results = 'hide'}
# Rerun the five layer model but now with 50 instead of 5 epochs
five_layer_model_50_epochs = five_layer_model.fit(train_images, train_labels, 
                                                  epochs = 50, validation_split = 0.3)
```

```{python}
# Compute the accuracy and loss of model performance on the test data
test_loss, test_acc = five_layer_model.evaluate(test_images, test_labels)
print("Model with five layers and fifty epochs -- Test loss:", test_loss * 100)
print("Model with five layers and fifty epochs -- Test accuracy:", test_acc * 100)
```

The five-layer model trained with fifty epochs has slightly higher accuracy than the same model trained with five epochs, but alsmost twice as much loss, while also being less time efficient. The predictions returned are probabilities per class. We can calculate the majority vote by taking class that has the maximum of predicted probabilities of all classes.

```{python, message = FALSE, warning = FALSE, results = 'hide'}
# Compute the predictions from the five layer model with 50 epochs
predictions = five_layer_model.predict(test_images)
majority_vote = dict()
for i in range(len(predictions)):
  majority_vote[i] = np.argmax(predictions[i])
```

We just saw that increasing the number of epochs from five to fifty slightly improved performance while substantially increasing loss. Therefore, we plot model loss and accuracy over the number of epochs for the training and cross-validation data to understand the trade-off between minimizing loss and maximizing accuracy.

```{python}
# Plot loss as function of epochs
plt.subplot(1, 2, 1)
plt.plot(five_layer_model_50_epochs.history['val_loss'], 'blue')
plt.plot(five_layer_model_50_epochs.history['loss'], 'red')
plt.legend(['Cross-validation data', 'Training data'], loc = 'upper left')
plt.ylabel('Loss')
plt.xlabel('Epoch')

# Plot accuracy as function of epochs
plt.subplot(1, 2, 2)
plt.plot(five_layer_model_50_epochs.history['val_acc'], 'blue')
plt.plot(five_layer_model_50_epochs.history['acc'], 'red')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.subplots_adjust(wspace = .35)

# Include plot title and show the plot
plt.suptitle('Model loss and accuracy over epochs for a five-layer neural network')
plt.show()
```

We observe that for the training data, loss decreases to zero while accuracy increases to one, as a result of overfitting (i.e. the model is fitted too well to a particular data set and therefore does not well extend to other data sets). This is why we also check how the model performs on the cross-validation data, for which we observe that both loss and accuracy increase with the number of epochs. Looking at the cross-validation data accuracy, we see that the peak lays at 0.896 for 18 epochs, and that for these number of epochs the loss is 0.382, which is much lower than for fifty epochs.

## Next up...
Next up in this series of blog posts, I will experiment with tree-based methods and support vector machines to see if they perform as well as the neural network in predicting clothing categories in the Fashion MNIST data. [Let's go!](https://github.com/fverkroost/RStudio-Blogs/blob/master/machineLearningFashionMNIST2.Rmd)



