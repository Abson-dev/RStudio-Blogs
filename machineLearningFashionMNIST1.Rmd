---
title: "A comparison of methods for predicting clothing classes using the Fashion MNIST dataset in RStudio and Python (Part 1)"
author: "Florianne Verkroost"
date: "23/08/2019"
output: html_document
---
  
In this series of blog posts, I will compare different machine and deep learning methods to predict clothing categories from images using the Fashion MNIST data. In this first blog of the series, we will explore and prepare the data for analysis. I will also show you how to perform K-means clustering as an exploratory analysis, to get an idea of what categories are more similar and dissimilar and thus more or less likely to be misclassified. In the [second blog post](https://github.com/fverkroost/RStudio-Blogs/blob/master/machineLearningFashionMNIST2.Rmd), I will show you how to predict the clothing categories of the Fashion MNIST data using my go-to model: an artificial neural network. Here, I will also show you how to use one of RStudios incredible features to run Python from RStudio. In the [third blog post](https://github.com/fverkroost/RStudio-Blogs/blob/master/machineLearningFashionMNIST3.Rmd), we will experiment with tree-based methods (single tree, random forests and boosting) and support vector machines to see whether we can beat the neural network in terms of performance. 

To start, we first need to install and load the required packages into RStudio and initialize the session such that we can later run Python in RStudio.

```{r setup, message = FALSE, warning = FALSE, results = 'hide'}
python3_path <- "/Library/Frameworks/Python.framework/Versions/3.7/bin/python3"
knitr::opts_chunk$set(echo = TRUE, engine.path = list(python = python3_path))
library(reticulate)    # run Python in R
library(devtools)
library(tensorflow)    # backend for keras
install_tensorflow()
library(keras)         # obtaining the data
install_keras()        
library(reshape2)      # melting data into long format
library(plyr)          # data manipulation
library(dplyr)         # data manipulation
library(fpc)           # cluster validation statistics
library(ggplot2)       # visualize and plot
library(rpart)         # run single tree models
library(randomForest)  # run random forest models
library(gbm)           # run boosting models
library(e1071)         # run support vector machines
library(caret)         # calculate variable importance
use_python(python3_path)
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
set.seed(1234)
```

## Importing and exploring the data

The *Keras* package contains the Fashion MNIST data, so we can easily import the data into RStudio from this package directly.

```{r}
fashion_mnist <- keras::dataset_fashion_mnist()
```

The resulting object named *fashion_mnist* is a nested list, consisting of lists named *train* and *test*. Each of these lists in turn consists of arrays *x* and *y*. To look at the dimentions of these elements, we recursively apply the *dim()* function to the *fashion_mnist* list.

```{r}
rapply(fashion_mnist, dim)
```

From the result, we observe that the *x* array in the training data contains 28 matrices each of 60000 rows and 28 columns, or in other words 60000 images each of 28 by 28 pixels. The *y* array in the training data contains 60000 labels for each of the images in the *x* array of the training data. The test data has a similar structure but only contains 10000 images rather than 60000. For simplicity, we rename these lists elements to something more intuitive (where *x* now represents images and *y* represents labels):
  
```{r}
c(train.images, train.labels) %<-% fashion_mnist$train
c(test.images, test.labels) %<-% fashion_mnist$test
```

Every image is captured by a 28 by 28 matrix, where entry [i, j] represents the opacity of that pixel on an integer scale from zero (white) to 255 (black). The labels consist of integers between zero and nine, each representing a unique clothing category. As the category names are not contained in the data itself, we have to store and add them manually. Note that the categories are evenly distributed in the data.

```{r}
cloth_cats = data.frame(category = c('Top', 'Trouser', 'Pullover', 'Dress', 'Coat',  
                                     'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Boot'), 
                        label = seq(0, 9))
```

To get an idea of what the data entail and look like, we plot the first ten images of the test data. To do so, we first need to reshape the data slightly such that it becomes compatible with *ggplot2*. We select the first ten test images, convert them to data frames, rename the columns into digits 1 to 28, create a variable named *y* with digits 1 to 28 and then we melt by variable *y*. This results in a 28 times 28 equals 784 by 3 (y pixels (= y), x pixels (= variable) and the opacity (= value)) data frame. We bind these all together by rows and add a variable *Image*, which is a unique string repeated 784 times for each of the nine images containing the image number and corresponding test set label.

```{r}
subarray <- apply(test.images[1:10, , ], 1, as.data.frame)
subarray <- lapply(subarray, function(df){
  colnames(df) <- seq_len(ncol(df))
  df['y'] <- seq_len(nrow(df))
  df <- melt(df, id = 'y')
  return(df)
})
plotdf <- rbind.fill(subarray)
first_ten_labels <- cloth_cats$category[match(test.labels[1:10], cloth_cats$label)]
first_ten_categories <- paste0('Image ', 1:10, ': ', first_ten_labels)
plotdf['Image'] <- rep(first_ten_categories, unlist(lapply(subarray, nrow)))
```

We then plot these first ten test images. Note that we reverse the scale of the y-axis because the original dataset contains the images upside-down. We further remove the legend and axis labels and change the tick labels.

```{r}
g <- ggplot()
g <- g + geom_raster(data = plotdf, aes(x = variable, y = y, fill = value))
g <- g + facet_wrap(~ Image, nrow = 2, ncol = 5)
g <- g + scale_fill_gradient(low = "white", high = "black", na.value = NA)
g <- g + theme(aspect.ratio = 1, legend.position = "none") 
g <- g + labs(x = NULL, y = NULL)
g <- g + scale_x_discrete(breaks = seq(0, 28, 7), expand = c(0, 0))
g <- g + scale_y_reverse(breaks = seq(0, 28, 7), expand = c(0, 0))
g
```

## Data Preparation

Next, it's time to start the more technical work of predicting the labels from the image data. First, we need to reshape our data to convert it from a multidimensional array into a two-dimensional matrix. To do so, we vectorize each 28 by 28 matrix into a column of length 784, and then we bind the columns for all images on top of each other, finally taking the transpose of the resulting matrix. This way, we can convert a 28 by 28 by 60000 array into a 60000 by 784 matrix. We also normalize the data by dividing between the maximum opacity of 255.

```{r}
train.images <- data.frame(t(apply(train.images, 1, c))) / 255
test.images <- data.frame(t(apply(test.images, 1, c))) / 255
```

We also create two data frames that include all training and test data (images and labels), respectively.

```{r}
names(train.images) <- names(test.images) <- paste0('pixel', 1:784)
train.labels <- data.frame(label = factor(train.labels))
test.labels <- data.frame(label = factor(test.labels))
train.data <- cbind(train.labels, train.images)
test.data <- cbind(test.labels, test.images)
```
Finally, we prepare a balanced subsample of 1000 images (100 per clothing category) of the training data for later on. As some of the models take quite long, we fit them on a subsample of the training data due to time constraints. Fitting the models on the full training dataset is preferable though.
```{r}
train.data.subsample <- train.data %>% 
  group_by(label) %>% 
  sample_n(size = 100) %>% 
  ungroup()
train.images.subsample <- train.data.subsample %>% select(-one_of("label"))
train.labels.subsample <- train.data.subsample %>% select(one_of("label"))
```

## K-means Clustering

We start the analysis by means of a cluster analysis, using K-means clustering. Here, I choose K-means clustering rather than hierarchical clustering because the former is more time-efficient and performs better than the latter when the data are large, as is the case for the Fashion MNIST data. K-means clustering partitions a data set in K distinct, non-overlapping clusters, where each data point belongs to at least one of K clusters. A good clustering maximizes inter- cluster variance and minimizes intra-cluster variance. For a more formal explanation of K-means clustering, I refer you to 'An Introduction to Statistical Learning' by James, Witten, Hastie and Tibshirani (2011).

The largest difficulty with K-means clustering is choosing the number of clusters, K. To find the best suitable value of K, we use three measures: *Within Sum of Squares (SSE)*, *Calinski-Harabasz (CH)* (ratio of the mean between sum of squares and the within sum of squares), and the *Average Silhouette Width (ASW)* (measure computed from the average distance (e.g. Mahalanobis or Euclidian) of a point to all other points in the same cluster and the minimal average distance of a point to all points in other clusters). A "good" clustering is obtained for K where the SSW contains an elbow; where CH is largest; and where ASW is close to 1. We plot the values of these measures across different values of K, varying between two and ten, as one cluster does not make sense and neither do more then ten as our data only contain ten categories. We start by computing the three measures for different values of K and capturing these in a data frame. We do so on the balanced subsample because of time and computational constraints.

```{r}
kRange <- 2:10
meas.out.ch <- meas.out.sse <- meas.out.asw <- NULL
for (i in kRange){
  out <- kmeans(train.images.subsample, i, nstart = 100, iter.max = 20)
  clusmeas <- cluster.stats(dist(train.images.subsample), out$cluster)
  meas.out.ch <- rbind(meas.out.ch, clusmeas$ch)
  meas.out.sse <- rbind(meas.out.sse, clusmeas$within.cluster.ss)
  meas.out.asw <- rbind(meas.out.asw, clusmeas$avg.silwidth)
}
meas.out <- data.frame(K = kRange, ch = meas.out.ch, 
                       sse = meas.out.sse, asw = meas.out.asw)
meas.melt <- melt(meas.out, id = 'K')
var <- meas.melt$variable
meas.melt$variable <- ifelse(var == 'asw', 'Average Silhouette Width',
                             ifelse(var == 'sse', 'Within Sum of Squares',
                                    ifelse(var == "ch", 'Calinski-Harabasz', NA)))
```

Next, we can plot the three measures across values for K side by side, as follows.

```{r}
g <- ggplot(data = meas.melt, aes(x = K, y = value))
g <- g + facet_wrap(~ variable, scales = 'free_y')
g <- g + geom_point()
g <- g + geom_line()
g <- g + theme()
g <- g + labs(x = 'K', y = 'Value')
g <- g + theme(panel.grid.major = element_blank(), 
               panel.grid.minor = element_blank(),
               panel.background = element_blank(), 
               axis.line = element_line(colour = "black"))
g <- g + scale_x_continuous(breaks = kRange)
g
```

On the basis of these measures, it seems that ASW selects K = 2 or K = 4, CH selects K = 2 and SSE selects K = 4. Therefore, we decide to use K = 2,..., 4 clusters for now. To see what clusters are formed, we need to combine all the images from each cluster. In function *averages*, for each cluster k = 1,..., K, we calculate the average value of each of the 784 pixels over the images in the training data that have been classified as belonging to cluster k.

```{r}
averages <- function(j, object, k, data){
  v <- object$cluster
  m <- data[which(v == j), ]
  m <- sapply(m, as.numeric)
  cm <- colMeans(m)
  df <- data.frame(y = rep(1:28, 28), x = rep(1:28, each = 28), opacity = cm)
  names(df)[3] <- paste0('Cluster', j)
  return(df)
}
```
Using function *averages*, we obtain our clusters for K = 2,..., 4 and we reshape these data into a suitable plotting format.

```{r}
k <- c(2:4)
res.list <- list()
for (r in 1:length(k)){
  clust.out <- kmeans(train.images.subsample, k[r], nstart = 100, iter.max = 20)
  res <- data.frame()
  for (i in 1:k[r]){
    if (i == 1){
      res <- averages(i, clust.out, k[r], train.images.subsample)
    } else {
      res <- merge(res, averages(i, clust.out, k[r], train.images.subsample), 
                   by = c('y', 'x'))
    }
  }
  res.list[[r]] <- melt(res, id = c('y', 'x'))
}
```

To visualize what clusters have been formed, we plot our clusters in a similar fashion as we did earlier.

```{r}
g <- ggplot()
g <- g + facet_grid(~ variable)
g <- g + scale_fill_gradient(low = "white", high = "black", na.value = NA)
g <- g + theme(aspect.ratio = 1, legend.position = "none") 
g <- g + labs(x = NULL, y = NULL)
g <- g + scale_x_continuous(breaks = seq(0, 28, 7), expand = c(0, 0))
g <- g + scale_y_reverse(breaks = rev(seq(0, 28, 7)), expand = c(0, 0))
for (i in 1:length(res.list)){
  print(g + geom_raster(data = res.list[[i]], aes(x = x, y = y, fill = value)))
}
```

We observe that when choosing K = 2, it seems that a cluster is being formed that is more related to shoes and bags (horizontal items) and another cluster that is more related to shirts, pullovers and trousers (vertical items). These clusters become more distinguished when increasing K. For example, in the results for K = 4, we observe that the clusters seem to relate to boots and bags (1), trousers (2), shirts, pullovers and coats (3) and shoes (4). Between clusters, items are dissimilar in terms of shape (e.g. trousers and shoes), and within clusters items are more similar in terms of shape (e.g. shirts, pullovers and coats).


## Next up...
Next up in this series of blog posts, I show you how to predict clothing categories in the Fashion MNIST data by running a Python neural network in RStudio. [Let's go!](https://github.com/fverkroost/RStudio-Blogs/blob/master/machineLearningFashionMNIST2.Rmd)


